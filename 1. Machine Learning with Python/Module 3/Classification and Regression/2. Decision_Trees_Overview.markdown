# Decision Trees for Machine Learning

This guide provides an overview of Decision Trees, a supervised machine learning algorithm used for classification. It covers the definition, construction, training process, and key concepts such as splitting criteria and pruning, based on the provided transcript.

## What is a Decision Tree?

- **Definition**: A Decision Tree is a machine learning algorithm visualized as a flowchart for classifying data points.
- **Structure**:
  - **Internal Nodes**: Represent a test or condition on a feature (e.g., "Is age > 40?").
  - **Branches**: Represent the outcome of the test (e.g., "Yes" or "No").
  - **Leaf Nodes (Terminal Nodes)**: Assign a class label to the data (e.g., "Prescribe Drug A").
- **Purpose**: Classifies data by making a series of decisions based on feature values, leading to a final class prediction.

## Example: Medical Study for Drug Prescription

- **Scenario**: A researcher collects data on patients with the same illness, where each patient responded to one of two medications (Drug A or Drug B).
- **Dataset Features**:
  - Age (young, middle-aged, senior).
  - Gender (male, female).
  - Blood pressure.
  - Cholesterol (high, normal).
- **Target**: The drug (A or B) each patient responded to.
- **Goal**: Build a Decision Tree to predict the appropriate drug for a new patient with the same illness.
- **Decision Tree Example**:
  - Start with age:
    - Middle-aged → Prescribe Drug B.
    - Young → Check gender:
      - Male → Prescribe Drug B.
      - Female → Prescribe Drug A.
    - Senior → Check cholesterol:
      - Normal → Prescribe Drug B.
      - High → Prescribe Drug A.
- **Application**: The trained Decision Tree uses historical data to predict the drug class for an unknown patient.

## Building a Decision Tree

- **Process**: Decision Trees are built by considering the features of a dataset one by one, using recursive partitioning to classify the data.
- **Steps**:
  1. **Start with a Seed Node**: Begin with all labeled training data at the root node.
  2. **Select Best Feature**: Choose the feature that best splits the data into pre-labeled classes based on a splitting criterion.
  3. **Partition Data**: Split the data into subsets based on the feature’s test, creating branches to new nodes.
  4. **Repeat**: For each new node, select the next best feature to split the data, using each feature only once.
  5. **Stop**: Continue until a stopping criterion is met.

## Stopping Criteria (Pre-emptive Tree Pruning)

A Decision Tree stops growing when one of the following criteria is met:
- **Maximum Tree Depth**: The tree reaches a predefined depth limit.
- **Minimum Data Points in a Node**: A node contains fewer than a specified number of data points.
- **Minimum Samples in a Leaf**: A leaf contains fewer than a specified number of samples.
- **Maximum Leaf Nodes**: The tree reaches a predefined number of leaf nodes.
- **Performance-Based Pruning**: Branches that do not significantly improve performance are cut to simplify the tree.

## Pruning a Decision Tree

- **Purpose**: Pruning simplifies the Decision Tree to prevent overfitting and improve generalization.
- **Reasons for Pruning**:
  - **Overfitting**: A complex tree may capture noise or irrelevant details in the training data.
  - **Too Many Classes/Features**: Increases the risk of modeling noise rather than meaningful patterns.
- **Benefits**:
  - Simplifies the model, making it more concise and interpretable.
  - Improves predictive accuracy on new data.
  - Enhances generalization to unseen data.

## Splitting Criteria

To train a Decision Tree, the algorithm selects the feature that best splits the data at each node. Common splitting criteria include:

### Entropy and Information Gain

- **Entropy**:
  - Measures the randomness or disorder in a node’s classes.
  - Formula: Entropy = - [ p_A * log(p_A) + p_B * log(p_B) ], where p_A and p_B are the proportions of classes (e.g., Drug A and Drug B) in the node.
  - Interpretation:
    - Entropy = 0: Node is completely homogeneous (all data belongs to one class).
    - Entropy = 1: Node is equally divided between classes (e.g., p_A = p_B = 0.5).
  - Goal: Minimize entropy to create pure nodes.
- **Information Gain**:
  - Measures the reduction in entropy after splitting on a feature.
  - Formula: Information Gain = Entropy(before split) - Weighted Entropy(after split).
  - Example: Splitting on cholesterol yields an information gain of 0.042.
  - Goal: Maximize information gain to select the feature that best reduces uncertainty.

### Gini Impurity

- **Definition**: Measures the impurity of a node, or the likelihood of misclassifying a randomly chosen data point.
- **Goal**: Minimize Gini impurity to create pure nodes.
- **Usage**: An alternative to entropy, often simpler to compute.

### Example: Choosing the Best Feature

- **Dataset**: 14 patients with features (cholesterol, sex, etc.) and target (Drug A or B).
- **Splitting on Cholesterol**:
  - Splits data into high and normal cholesterol nodes.
  - Result: Impure nodes (mixed classes), low information gain (e.g., 0.042), indicating cholesterol is not the best feature.
- **Splitting on Sex**:
  - Splits data into male and female nodes.
  - Result: Female node is mostly Drug B (purer), male node requires further splitting (e.g., on cholesterol) to reach pure nodes.
  - Higher information gain, indicating sex is a better feature to split on.
- **Process**: The algorithm continues splitting until all nodes are pure or a stopping criterion is met.

## How Decision Trees Learn

- **Recursive Partitioning**:
  - The algorithm recursively splits the data based on the best feature at each node, creating a tree structure.
  - Each split aims to reduce impurity (entropy or Gini) and increase class purity.
- **Feature Selection**:
  - At each node, the algorithm evaluates all available features and selects the one that maximizes information gain or minimizes impurity.
  - Features are used only once along a branch to avoid redundancy.
- **Outcome**:
  - The tree grows until all leaf nodes contain a single class or a stopping criterion is met.
  - The resulting tree can predict the class of new data points by following the decision path from root to leaf.

## Advantages of Decision Trees

- **Visualization**: Decision Trees can be visualized as flowcharts, making them highly interpretable.
- **Feature Importance**: The order of feature splits reveals the predictive power of each feature, providing insights into the dataset.
- **Interpretability**: The decision-making process is transparent, showing exactly how predictions are made.

## Key Takeaways

- **Decision Tree Overview**:
  - A flowchart-like algorithm for classifying data, with internal nodes (tests), branches (test outcomes), and leaf nodes (class labels).
- **Building Process**:
  - Constructed by recursively splitting data based on the best feature, using criteria like information gain or Gini impurity.
- **Training**:
  - Grows from a seed node by selecting features that maximize class purity until a stopping criterion is met.
- **Pruning**:
  - Simplifies the tree to prevent overfitting, improve accuracy, and enhance generalization.
- **Splitting Criteria**:
  - **Entropy**: Measures randomness; minimized for pure nodes.
  - **Information Gain**: Measures entropy reduction; maximized for optimal splits.
  - **Gini Impurity**: Measures node impurity; minimized for pure nodes.
- **Applications**:
  - Useful for tasks like predicting drug prescriptions based on patient features, with clear visualization and interpretable results.

Decision Trees are powerful, interpretable tools for classification, enabling predictive modeling and feature analysis in machine learning applications.