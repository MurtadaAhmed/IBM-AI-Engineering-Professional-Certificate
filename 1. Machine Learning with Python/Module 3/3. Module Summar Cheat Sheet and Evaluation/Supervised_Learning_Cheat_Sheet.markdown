# Cheat Sheet: Building Supervised Learning Models

This cheat sheet summarizes common **supervised learning models** and **associated functions** for building machine learning models. It includes brief descriptions, code syntax, key hyperparameters, pros, cons, and applications, formatted for beginners with clear explanations of technical terms.

## Common Supervised Learning Models

| **Process Name** | **Brief Description** | **Code Syntax** | **Key Hyperparameters** | **Pros** | **Cons** | **Common Applications** |
|-------------------|-----------------------|-----------------|-------------------------|----------|----------|-------------------------|
| **One vs One Classifier (using Logistic Regression)** | Trains a separate classifier for each pair of classes to handle **multiclass classification** (predicting one of many categories). *Clarification*: Compares every class against every other class (e.g., dog vs. cat, dog vs. bird, cat vs. bird). | ```python<br>from sklearn.multiclass import OneVsOneClassifier<br>from sklearn.linear_model import LogisticRegression<br>model = OneVsOneClassifier(LogisticRegression())<br>``` | - `estimator`: Base classifier (e.g., LogisticRegression)<br>*Clarification*: The base classifier is the model used for each pair. | Works well for small datasets. | Computationally expensive for large datasets (many pairs to compute). | Multiclass classification with a small number of classes (e.g., handwriting recognition). |
| **One vs All Classifier (using Logistic Regression)** | Trains one classifier per class, distinguishing it from all other classes combined (also called One-vs-Rest). *Clarification*: Each classifier checks "Is it this class or not?" | ```python<br>from sklearn.multiclass import OneVsRestClassifier<br>from sklearn.linear_model import LogisticRegression<br>model = OneVsRestClassifier(LogisticRegression())<br>```<br>*or*<br>```python<br>from sklearn.linear_model import LogisticRegression<br>model_ova = LogisticRegression(multi_class='ovr')<br>``` | - `estimator`: Base classifier<br>- `multi_class`: Set to `ovr` for One-vs-Rest<br>*Clarification*: `ovr` tells the model to use One-vs-Rest strategy. | Simpler and more scalable than One-vs-One. | Less accurate for **imbalanced classes** (when some classes have much more data). | Multiclass classification, such as image classification (e.g., identifying objects in photos). |
| **Decision Tree Classifier** | A tree-based model that splits data into smaller groups based on feature values to classify data. *Clarification*: Like a flowchart of yes/no questions (e.g., "Is age > 30?"). | ```python<br>from sklearn.tree import DecisionTreeClassifier<br>model = DecisionTreeClassifier(max_depth=5)<br>``` | - `max_depth`: Maximum number of splits (levels) in the tree<br>*Clarification*: Limits how detailed the tree gets to avoid overfitting. | Easy to interpret and visualize (like a flowchart). | Prone to **overfitting** (memorizing data) if not **pruned** (simplified). | Classification tasks, such as credit risk assessment (e.g., approve/deny loans). |
| **Decision Tree Regressor** | Like a decision tree classifier but predicts numbers (continuous values) instead of categories. *Clarification*: Splits data to find average values in groups. | ```python<br>from sklearn.tree import DecisionTreeRegressor<br>model = DecisionTreeRegressor(max_depth=5)<br>``` | - `max_depth`: Maximum depth of the tree<br>*Clarification*: Controls complexity to prevent overfitting. | Easy to interpret, handles **nonlinear data** (curvy patterns). | Can overfit and struggle with noisy data (random errors). | Regression tasks, such as predicting housing prices. |
| **Linear SVM Classifier** | Finds a straight line (or **hyperplane**) that separates classes with the widest possible gap (margin). *Clarification*: Like drawing a line to keep two groups apart. | ```python<br>from sklearn.svm import SVC<br>model = SVC(kernel='linear', C=1.0)<br>``` | - `C`: Controls strictness of separation<br>- `kernel`: Set to `linear` for straight-line separation<br>- `gamma`: Used for non-linear kernels (not linear)<br>*Clarification*: `C` balances accuracy vs. flexibility. | Effective for **high-dimensional data** (many features). | Not great for nonlinear data without a **kernel trick** (transforming data). | Text classification, image recognition. |
| **K-Nearest Neighbors Classifier** | Classifies a data point based on the most common class among its closest neighbors. *Clarification*: Like asking your nearest friends to vote on a category. | ```python<br>from sklearn.neighbors import KNeighborsClassifier<br>model = KNeighborsClassifier(n_neighbors=5, weights='uniform')<br>``` | - `n_neighbors`: Number of neighbors (K)<br>- `weights`: `uniform` (equal votes) or `distance` (closer neighbors count more)<br>- `algorithm`: Method to find neighbors (e.g., `auto`)<br>*Clarification*: `K` decides how many neighbors to check. | Simple and effective for small datasets. | Slow and memory-heavy for large datasets. | Recommendation systems, image recognition. |
| **Random Forest Regressor** | Combines many decision trees (an **ensemble**) to predict numbers, reducing overfitting. *Clarification*: Like averaging predictions from a team of trees. | ```python<br>from sklearn.ensemble import RandomForestRegressor<br>model = RandomForestRegressor(n_estimators=100, max_depth=5)<br>``` | - `n_estimators`: Number of trees<br>- `max_depth`: Maximum depth of each tree<br>*Clarification*: More trees improve stability but increase computation. | Less likely to overfit than single decision trees. | Complex and slower with many trees. | Regression tasks, such as predicting sales or stock prices. |
| **XGBoost Regressor** | A **gradient boosting** method that builds trees sequentially, each correcting the previous one’s errors. *Clarification*: Like a team where each member learns from the last one’s mistakes. | ```python<br>import xgboost as xgb<br>model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5)<br>``` | - `n_estimators`: Number of trees<br>- `learning_rate`: How much each tree corrects errors<br>- `max_depth`: Maximum tree depth<br>*Clarification*: `learning_rate` controls how fast the model learns. | Highly accurate, great for large datasets. | Computationally intensive, hard to tune. | Predictive modeling, especially in competitions like Kaggle. |

## Associated Functions Used

| **Method Name** | **Brief Description** | **Code Syntax** |
|-----------------|-----------------------|-----------------|
| **OneHotEncoder** | Converts categorical features (e.g., "red," "blue") into a numeric format (0s and 1s) for models. *Clarification*: Turns categories into columns where only one is "1" (e.g., red=1, blue=0). | ```python<br>from sklearn.preprocessing import OneHotEncoder<br>encoder = OneHotEncoder(sparse=False)<br>encoded_data = encoder.fit_transform(categorical_data)<br>``` |
| **accuracy_score** | Measures how many predictions a classifier got right (percentage correct). *Clarification*: Like checking how many test answers you scored correctly. | ```python<br>from sklearn.metrics import accuracy_score<br>accuracy = accuracy_score(y_true, y_pred)<br>``` |
| **LabelEncoder** | Converts categorical labels (e.g., "dog," "cat") into numbers (e.g., 0, 1). *Clarification*: Simplifies labels for models that need numbers. | ```python<br>from sklearn.preprocessing import LabelEncoder<br>encoder = LabelEncoder()<br>encoded_labels = encoder.fit_transform(labels)<br>``` |
| **plot_tree** | Draws a decision tree to visualize its structure. *Clarification*: Shows the flowchart of questions the tree asks. | ```python<br>from sklearn.tree import plot_tree<br>plot_tree(model, max_depth=3, filled=True)<br>``` |
| **normalize** | Adjusts features to have a mean of 0 and a standard deviation of 1 (standardization). *Clarification*: Makes all features equally important by putting them on the same scale. | ```python<br>from sklearn.preprocessing import normalize<br>normalized_data = normalize(data, norm='l2')<br>``` |
| **compute_sample_weight** | Assigns weights to data points to handle **imbalanced datasets** (when some classes have more data). *Clarification*: Gives more importance to underrepresented classes. | ```python<br>from sklearn.utils.class_weight import compute_sample_weight<br>weights = compute_sample_weight(class_weight='balanced', y=y)<br>``` |
| **roc_auc_score** | Measures the quality of a binary classifier using the Area Under the ROC Curve (AUC-ROC). *Clarification*: Higher AUC means better at distinguishing classes (e.g., spam vs. not spam). | ```python<br>from sklearn.metrics import roc_auc_score<br>auc = roc_auc_score(y_true, y_score)<br>``` |

## Notes for Beginners

- **Supervised Learning**: Models learn from **labeled data** (data with known answers, like pictures labeled "cat" or "dog") to predict labels for new data.
- **Hyperparameters**: Settings you choose before training (e.g., `max_depth`, `n_neighbors`) that control how the model learns. *Clarification*: Like adjusting settings on a game to make it easier or harder.
- **Overfitting**: When a model learns the training data too well, including **noise** (random errors), and fails on new data. *Clarification*: Like memorizing a test but failing a new quiz.
- **Imbalanced Classes**: When some categories have much more data (e.g., 90% "not spam," 10% "spam"), making predictions biased toward the majority. *Clarification*: Like a vote where one group has more people.
- **Ensemble**: Combining multiple models to improve predictions (e.g., Random Forest, XGBoost). *Clarification*: Like a team of experts giving a combined opinion.
- **Scikit-learn**: A Python library with tools for building machine learning models (most code examples above use it).
- **XGBoost**: A separate library for advanced boosting models, popular for high accuracy.

**Authors**: Jeff Grossman, Abhishek Gagneja